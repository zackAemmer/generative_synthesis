{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "###### Build up fundamental model:\n",
    "-Test speeds vs traditional methods  \n",
    "-Energy distance test for distributions  \n",
    "-Check distributions of variables within households, compare to naive method w/borysov model somehow  \n",
    "-Test making epsilon the same distribution as the actual posteriors in the model\n",
    "\n",
    "###### Differentiate from the GenSynth paper:\n",
    "-Travel diaries  \n",
    "-Method/heuristic/rules for checking large number of attributes  \n",
    "-New models; Disentangled VAE/GAN  \n",
    "-Model population changes over time RNN?  \n",
    "-Behavioral variables  \n",
    "\n",
    "###### They suggest:\n",
    "-Incorporate RNN to generate trip chains (time, location, mode, purpose)  \n",
    "-Use GAN/other method to generate less inconsistencies  \n",
    "-Address next stage of re-sampling to get future populations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input to training the model is a person's daily trip diary\n",
    "# Inputs; day of week, characteristics of person/hh\n",
    "# Outputs; trip purpose, mode, duration, distance\n",
    "# How to include Time of Day?\n",
    "# Timesteps could either be hours in the day, or trips in a chain?\n",
    "    # If a timestep is a trip, add the time of departure to the output variables\n",
    "    \n",
    "# Timestep is a trip\n",
    "# Each timestep has departure time, duration, distance, mode, and purpose (y)\n",
    "# Each trip has person/hh variables, day of week, and previous timestep info (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skpre\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the persons PUMS dataset for WA state\n",
    "t_df = pd.read_csv('data/NHTS/nhts_survey/trippub.csv')\n",
    "p_df = pd.read_csv('data/NHTS/nhts_survey/perpub.csv')\n",
    "h_df = pd.read_csv('data/NHTS/nhts_survey/hhpub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Variables and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset n=923572 pre-cleaning\n",
      "Dataset n=405590 post-cleaning\n"
     ]
    }
   ],
   "source": [
    "# Filter to desired variables (numeric then categorical)\n",
    "#TRIPPURP = simplified why/from\n",
    "nhts_data_t = t_df[['TDCASEID','HOUSEID','PERSONID','TDAYDATE','TRAVDAY','TDTRPNUM','STRTTIME','TRVLCMIN','TRPMILES','TRPTRANS','WHYFROM','WHYTO']]\n",
    "nhts_data_p = p_df[['HOUSEID','PERSONID','R_AGE','TIMETOWK','EDUC','R_SEX','OCCAT']]\n",
    "nhts_data_h = h_df[['HOUSEID','HHSIZE','HHFAMINC','HHVEHCNT']]\n",
    "del t_df\n",
    "del p_df\n",
    "del h_df\n",
    "nhts_data = pd.merge(nhts_data_t, nhts_data_h, on='HOUSEID', how='left')\n",
    "nhts_data = pd.merge(nhts_data, nhts_data_p, on=['HOUSEID', 'PERSONID'], how='left')\n",
    "\n",
    "# Give each set of daily trips a unique chain id (each will be an input to model)\n",
    "chain_ids = nhts_data.groupby(['TDAYDATE','HOUSEID','PERSONID']).ngroup().values\n",
    "nhts_data = nhts_data.drop(labels=['TDAYDATE','TDCASEID','HOUSEID','PERSONID'], axis=1)\n",
    "nhts_data['CHAINID'] = chain_ids\n",
    "\n",
    "# Remove NA values and check n before/after\n",
    "print(f\"Dataset n={len(nhts_data)} pre-cleaning\")\n",
    "nan_indices = list((nhts_data < 0).any(axis=1))\n",
    "nan_ids = chain_ids[nan_indices]\n",
    "nhts_data = nhts_data[~(nhts_data['CHAINID'].isin(nan_ids))]\n",
    "chain_ids = nhts_data['CHAINID'].values\n",
    "print(f\"Dataset n={len(nhts_data)} post-cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variable class lengths: []\n"
     ]
    }
   ],
   "source": [
    "# Record constants for some characteristics of the dataset before coding vars\n",
    "CAT_IDX = 18\n",
    "STATIC_IDX = [0,1,2,3,4,5,6,7]\n",
    "DIM_DYNAMIC = nhts_data_t.shape[1] - 3  # P/H/T id\n",
    "DIM_STATIC = nhts_data_p.shape[1] + nhts_data_h.shape[1] - 3  # P/H/T id\n",
    "DIM_TOTAL = nhts_data.shape[1]\n",
    "VAR_NAMES = nhts_data.columns\n",
    "NUM_SAMPLES = len(pd.unique(chain_ids))\n",
    "\n",
    "# Split categorical data into OHE vars, save num classes per variable\n",
    "dummies_list = []\n",
    "for x in range(CAT_IDX, DIM_TOTAL):\n",
    "    dummies = nhts_data.iloc[:,x]\n",
    "    dummies = pd.get_dummies(dummies, prefix=f\"{nhts_data.columns[x]}_\")\n",
    "    dummies_list.append(dummies)\n",
    "CAT_LENGTHS = [x.shape[1] for x in dummies_list]\n",
    "print(f\"Categorical variable class lengths: {CAT_LENGTHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAVDAY</th>\n",
       "      <th>TDTRPNUM</th>\n",
       "      <th>STRTTIME</th>\n",
       "      <th>TRVLCMIN</th>\n",
       "      <th>TRPMILES</th>\n",
       "      <th>TRPTRANS</th>\n",
       "      <th>WHYFROM</th>\n",
       "      <th>WHYTO</th>\n",
       "      <th>HHSIZE</th>\n",
       "      <th>HHFAMINC</th>\n",
       "      <th>HHVEHCNT</th>\n",
       "      <th>R_AGE</th>\n",
       "      <th>TIMETOWK</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>R_SEX</th>\n",
       "      <th>OCCAT</th>\n",
       "      <th>CHAINID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>120</td>\n",
       "      <td>84.004</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>46938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1800</td>\n",
       "      <td>150</td>\n",
       "      <td>81.628</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>46938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1115</td>\n",
       "      <td>15</td>\n",
       "      <td>8.017</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2330</td>\n",
       "      <td>10</td>\n",
       "      <td>8.017</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>550</td>\n",
       "      <td>15</td>\n",
       "      <td>3.395</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>24626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923567</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>810</td>\n",
       "      <td>27</td>\n",
       "      <td>1.168</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923568</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1320</td>\n",
       "      <td>8</td>\n",
       "      <td>0.238</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923569</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1415</td>\n",
       "      <td>5</td>\n",
       "      <td>0.238</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923570</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1820</td>\n",
       "      <td>25</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923571</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1848</td>\n",
       "      <td>7</td>\n",
       "      <td>0.325</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405590 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TRAVDAY  TDTRPNUM  STRTTIME  TRVLCMIN  TRPMILES  TRPTRANS  WHYFROM  \\\n",
       "2             2         1       700       120    84.004         6        3   \n",
       "3             2         2      1800       150    81.628         6        1   \n",
       "6             5         1      1115        15     8.017         6        1   \n",
       "7             5         2      2330        10     8.017         6        3   \n",
       "8             5         1       550        15     3.395         4        1   \n",
       "...         ...       ...       ...       ...       ...       ...      ...   \n",
       "923567        3         1       810        27     1.168         1        1   \n",
       "923568        3         2      1320         8     0.238         1        3   \n",
       "923569        3         3      1415         5     0.238         1       16   \n",
       "923570        3         4      1820        25     0.867         1        3   \n",
       "923571        3         5      1848         7     0.325         1       12   \n",
       "\n",
       "        WHYTO  HHSIZE  HHFAMINC  HHVEHCNT  R_AGE  TIMETOWK  EDUC  R_SEX  \\\n",
       "2           1       3         7         5     66       120     3      1   \n",
       "3           3       3         7         5     66       120     3      1   \n",
       "6           3       2         8         4     55        10     5      1   \n",
       "7           1       2         8         4     55        10     5      1   \n",
       "8          16       1        10         2     45        30     5      2   \n",
       "...       ...     ...       ...       ...    ...       ...   ...    ...   \n",
       "923567      3       1        10         0     52        20     5      1   \n",
       "923568     16       1        10         0     52        20     5      1   \n",
       "923569      3       1        10         0     52        20     5      1   \n",
       "923570     12       1        10         0     52        20     5      1   \n",
       "923571      1       1        10         0     52        20     5      1   \n",
       "\n",
       "        OCCAT  CHAINID  \n",
       "2           2    46938  \n",
       "3           2    46938  \n",
       "6           4    46940  \n",
       "7           4    46940  \n",
       "8           4    24626  \n",
       "...       ...      ...  \n",
       "923567      4    93638  \n",
       "923568      4    93638  \n",
       "923569      4    93638  \n",
       "923570      4    93638  \n",
       "923571      4    93638  \n",
       "\n",
       "[405590 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final data frame after encoding OHE\n",
    "model_data_df = nhts_data.iloc[:,:CAT_IDX]\n",
    "for ohe_var in dummies_list:\n",
    "    model_data_df = pd.concat([model_data_df, ohe_var], axis=1)\n",
    "DIM_MANIFEST = model_data_df.shape[1]\n",
    "\n",
    "# Preview data that will be fed into model\n",
    "model_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset n=405590 pre-numerical-outliers\n",
      "Dataset n=345658 post-numerical-outliers\n"
     ]
    }
   ],
   "source": [
    "# Standardize the input data from -1 to 1 for numerical variables\n",
    "scaler = skpre.StandardScaler()\n",
    "model_data = model_data_df.drop('CHAINID', axis=1).values\n",
    "model_data[:,:CAT_IDX] = scaler.fit_transform(model_data[:,:CAT_IDX])\n",
    "\n",
    "# Remove rows with numerical outliers (3 < standard deviations)\n",
    "print(f\"Dataset n={len(model_data)} pre-numerical-outliers\")\n",
    "outlier_indices = np.where(np.any(model_data[:,:CAT_IDX] > 3, axis=1))\n",
    "outlier_ids = chain_ids[outlier_indices]\n",
    "to_remove_indices = ~(model_data_df['CHAINID'].isin(outlier_ids))\n",
    "model_data_df = model_data_df[to_remove_indices]\n",
    "model_data = model_data[to_remove_indices]\n",
    "chain_ids = model_data_df['CHAINID'].values\n",
    "print(f\"Dataset n={len(model_data)} post-numerical-outliers\")\n",
    "\n",
    "# Separate chain ids into train/test data\n",
    "chain_ids = pd.unique(chain_ids)\n",
    "train_idx = round(len(chain_ids)*.9)\n",
    "train_data = chain_ids[0:train_idx]\n",
    "test_data = chain_ids[train_idx:len(chain_ids)]\n",
    "\n",
    "# Filter model data into train/test data\n",
    "train_data = model_data[model_data_df['CHAINID'].isin(train_data)]\n",
    "test_data = model_data[model_data_df['CHAINID'].isin(test_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters and Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARN_RATE = 0.001\n",
    "RHO = 0.9\n",
    "LATENT_DIM = 5\n",
    "HIDDEN_DIM = 100\n",
    "KL_WEIGHT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(LATENT_DIM + LEN_HH,))\n",
    "decoder_x = layers.Dense(HIDDEN_DIM, activation=\"tanh\")(decoder_inputs)\n",
    "decoder_x = layers.Dense(HIDDEN_DIM, activation=\"tanh\")(decoder_x)\n",
    "decoder_num_outputs = layers.Dense(CAT_IDX, activation=\"linear\")(decoder_x)\n",
    "decoder_cat_outputs = []\n",
    "for var_length in CAT_LENGTHS:\n",
    "    layer = layers.Dense(var_length, activation=\"softmax\")(decoder_x)\n",
    "    decoder_cat_outputs.append(layer)\n",
    "decoder = keras.Model(decoder_inputs, [decoder_num_outputs, decoder_cat_outputs], name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss function for combined numerical and categorical data\n",
    "def get_reconstruction_loss(data, reconstruction, CAT_IDX, CAT_LENGTHS):\n",
    "\n",
    "    # Mean squared error for numerical variables\n",
    "    data_num = data[:,:CAT_IDX]\n",
    "    reconstruction_num = reconstruction[0]\n",
    "    loss_num = keras.losses.mean_squared_error(data_num, reconstruction_num)\n",
    "    loss_num = tf.reduce_mean(loss_num)\n",
    "    \n",
    "    # Categorical cross entropy for categorical variables\n",
    "    loss_list = []\n",
    "    current = CAT_IDX\n",
    "    for i, x in enumerate(CAT_LENGTHS):\n",
    "        data_cat = data[:,current:(current + x)]\n",
    "        reconstruction_cat = reconstruction[1][i]\n",
    "        loss = keras.losses.categorical_crossentropy(data_cat, reconstruction_cat, from_logits=False)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        loss_list.append(loss)\n",
    "        current += x\n",
    "    loss_cat = tf.reduce_sum(loss_list)\n",
    "\n",
    "    # Return both losses; they are combined in the model\n",
    "    return (loss_num, loss_cat)\n",
    "\n",
    "def get_kl_loss(z_mean, z_log_var):\n",
    "    kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    kl_loss = -kl_loss\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss metric recorder\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "# Variational Autoencoder\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, CAT_IDX, CAT_LENGTHS, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.CAT_IDX = CAT_IDX\n",
    "        self.CAT_LENGTHS = CAT_LENGTHS\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get latent vars from the encoder; feed to decoder and get sampled manifest variables\n",
    "            z_mean, z_log_var, z = encoder(data[:,:MANIFEST_DIM])\n",
    "            decoder_input = tf.concat([z, data[:,MANIFEST_DIM:]], axis=1)\n",
    "            reconstruction = decoder(decoder_input)\n",
    "\n",
    "            # Get loss between input values and reconstruction\n",
    "            reconstruction_loss_num, reconstruction_loss_cat = get_reconstruction_loss(\n",
    "                data,\n",
    "                reconstruction,\n",
    "                self.CAT_IDX,\n",
    "                self.CAT_LENGTHS\n",
    "            )\n",
    "            reconstruction_loss = tf.add(reconstruction_loss_num, reconstruction_loss_cat)\n",
    "\n",
    "            # Get Kullback Leidler loss between normal distribution and actual for latent variables\n",
    "            kl_loss = get_kl_loss(z_mean, z_log_var)\n",
    "            kl_loss = kl_loss * KL_WEIGHT\n",
    "\n",
    "            # Combine into single loss term\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        # Get new gradients given the loss and take another step (update weights)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # Record the loss metrics\n",
    "        loss_tracker.update_state(total_loss)\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": loss_tracker.result(),\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"reconstruction_loss_num\": reconstruction_loss_num,\n",
    "            \"reconstruction_loss_cat\": reconstruction_loss_cat,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs[:,:MANIFEST_DIM])\n",
    "        decoder_input = tf.concat([z, inputs[:,MANIFEST_DIM:]], axis=1)\n",
    "        reconstruction = self.decoder(decoder_input)\n",
    "        return reconstruction\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "vae = VAE(encoder, decoder, CAT_IDX, CAT_LENGTHS)\n",
    "vae.compile(optimizer=keras.optimizers.RMSprop(learning_rate=LEARN_RATE, rho=RHO))\n",
    "history = vae.fit(train_data, epochs=EPOCHS, batch_size=BATCH_SIZE) #add callbacks=[callback] if debugging needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model loss/training progress\n",
    "plt.plot(history.history['total_loss'])\n",
    "plt.plot(history.history['reconstruction_loss_num'])\n",
    "plt.plot(history.history['reconstruction_loss_cat'])\n",
    "plt.plot(history.history['kl_loss'])\n",
    "plt.title(\"Training History\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"total\",\"rec_num\",\"rec_cat\",\"kl\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent vars from the encoder; feed to decoder and get sampled manifest variables\n",
    "z_mean, z_logvar, z = vae.encoder.predict(test_data[:,:MANIFEST_DIM])\n",
    "\n",
    "# Record the posterior trained distributions for z\n",
    "latent_means = []\n",
    "latent_vars = []\n",
    "\n",
    "# Determine the average values for the mean/logvariance of the latent variables\n",
    "for i in range(0, LATENT_DIM):\n",
    "    epsilon = np.random.normal(loc=0, scale=1, size=1000)\n",
    "    avg_mean = np.mean(z_mean[:,i])\n",
    "    latent_means.append(avg_mean)\n",
    "    avg_var = np.exp(np.mean(z_logvar[:,i]))\n",
    "    latent_vars.append(avg_var)\n",
    "    print(f\"Latent Variable: {i}\")\n",
    "    print(f\"Mean: {avg_mean}\")\n",
    "    print(f\"Variance: {np.exp(avg_var)}\\n\")\n",
    "    samples = avg_mean + (avg_var * epsilon)\n",
    "    plt.hist(samples, bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw predictions from test data\n",
    "results = vae.predict(test_data)\n",
    "loss_num, loss_cat = get_reconstruction_loss(test_data, results, CAT_IDX, CAT_LENGTHS)\n",
    "print(f\"Numerical Variable Loss: {loss_num}\")\n",
    "print(f\"Categorical Variable Loss: {loss_cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numeric results back to real variable values\n",
    "results_num = scaler_pers.inverse_transform(results[0])\n",
    "results_df = pd.DataFrame(results_num)\n",
    "\n",
    "# Transform categorical results back to real variable values\n",
    "for x in results[1]:\n",
    "    result = np.argmax(x, axis=1) + 1\n",
    "    results_df[f\"{x}\"] = result\n",
    "\n",
    "# Add back original variables names to the results\n",
    "results_df.columns = VAR_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numeric test data back to real variable values\n",
    "test_data_num = scaler_pers.inverse_transform(test_data[:,:CAT_IDX])\n",
    "test_data_df = pd.DataFrame(test_data_num)\n",
    "\n",
    "# Transform categorical test data back to real variable values\n",
    "current = CAT_IDX\n",
    "for x in CAT_LENGTHS:\n",
    "    test_data_cat = test_data[:,current:(current + x)]\n",
    "    test_data_cat = np.argmax(test_data_cat, axis=1) + 1\n",
    "    test_data_df[f\"{x}\"] = test_data_cat\n",
    "    current += x\n",
    "\n",
    "# Add back original variables names to the test data\n",
    "test_data_df.columns = VAR_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting numerical variables\n",
    "for col_idx in range(0, CAT_IDX):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "\n",
    "    plt.hist(results_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(test_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting categorical variables\n",
    "for col_idx in range(CAT_IDX, VAR_DIM):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "    \n",
    "    plt.hist(results_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(test_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list of households from the test data (in future can be generated with separate vae)\n",
    "x = pd.DataFrame(test_data[:,MANIFEST_DIM:]).reset_index(drop=True) # Scaled hh_input values\n",
    "y = model_data_df.iloc[train_idx:][['NP']].reset_index(drop=True) # Unscaled number of persons value\n",
    "z = pd.concat([x,y],axis=1)\n",
    "z.columns = ['HINCP','NP','VEH','SIZE']\n",
    "\n",
    "# Multiply the inputs by the number of persons per household (hh of size 3 becomes 3 rows with same scaled hh inputs)\n",
    "z = z.reindex(z.index.repeat(z['SIZE']))\n",
    "z = z[['HINCP','NP','VEH']].values\n",
    "\n",
    "# Generate random normal sample to represent each latent variable, for each row (different person per row)\n",
    "epsilon = np.random.normal(loc=0, scale=1, size=(len(z), LATENT_DIM))\n",
    "inputs = np.concatenate((epsilon, z), axis=-1)\n",
    "\n",
    "# Generate persons; each person has unique latent input, plus shared hh inputs with their household\n",
    "results = vae.decoder.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numeric results back to real variable values\n",
    "results_num = scaler_pers.inverse_transform(results[0])\n",
    "results_df = pd.DataFrame(results_num)\n",
    "\n",
    "# Transform categorical results back to real variable values\n",
    "for x in results[1]:\n",
    "    result = np.argmax(x, axis=1) + 1\n",
    "    results_df[f\"{x}\"] = result\n",
    "\n",
    "# Add back original variables names to the results\n",
    "results_df.columns = VAR_NAMES\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting numerical variables\n",
    "for col_idx in range(0, CAT_IDX):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "\n",
    "    plt.hist(results_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(test_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting categorical variables\n",
    "for col_idx in range(CAT_IDX, VAR_DIM):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "    \n",
    "    plt.hist(results_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(test_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
