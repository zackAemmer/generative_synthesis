{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "###### Build up fundamental model:\n",
    "-Test speeds vs traditional methods  \n",
    "-Energy distance test for distributions  \n",
    "-Check distributions of variables within households, compare to naive method w/borysov model somehow  \n",
    "-Test making epsilon the same distribution as the actual posteriors in the model\n",
    "\n",
    "###### Differentiate from the GenSynth paper:\n",
    "-Travel diaries  \n",
    "-Method/heuristic/rules for checking large number of attributes  \n",
    "-New models; Disentangled VAE/GAN  \n",
    "-Model population changes over time RNN?  \n",
    "-Behavioral variables  \n",
    "\n",
    "###### They suggest:\n",
    "-Incorporate RNN to generate trip chains (time, location, mode, purpose)  \n",
    "-Use GAN/other method to generate less inconsistencies  \n",
    "-Address next stage of re-sampling to get future populations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input to training the model is a person's daily trip diary\n",
    "# Inputs; day of week, characteristics of person/hh\n",
    "# Outputs; trip purpose, mode, duration, distance\n",
    "# How to include Time of Day?\n",
    "# Timesteps could either be hours in the day, or trips in a chain?\n",
    "    # If a timestep is a trip, add the time of departure to the output variables\n",
    "    \n",
    "# Timestep is a trip\n",
    "# Output of each timestep is departure time, duration, distance, mode, and purpose (y)\n",
    "# Input of each timestep is person/hh variables, day of week, and previous timestep info (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skpre\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the persons PUMS dataset for WA state\n",
    "t_df = pd.read_csv('data/NHTS/nhts_survey/trippub.csv')\n",
    "p_df = pd.read_csv('data/NHTS/nhts_survey/perpub.csv')\n",
    "h_df = pd.read_csv('data/NHTS/nhts_survey/hhpub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Variables and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset n=923572 pre-cleaning\n",
      "Dataset n=405590 post-cleaning\n"
     ]
    }
   ],
   "source": [
    "# Filter to desired variables (numeric then categorical)\n",
    "#TRIPPURP = simplified why/from\n",
    "nhts_data_t = t_df[['TDCASEID','HOUSEID','PERSONID','TDAYDATE','TRAVDAY','TDTRPNUM','STRTTIME','TRVLCMIN','TRPMILES','TRPTRANS','WHYFROM','WHYTO']]\n",
    "nhts_data_p = p_df[['HOUSEID','PERSONID','R_AGE','TIMETOWK','EDUC','R_SEX','OCCAT']]\n",
    "nhts_data_h = h_df[['HOUSEID','HHSIZE','HHFAMINC','HHVEHCNT']]\n",
    "del t_df\n",
    "del p_df\n",
    "del h_df\n",
    "nhts_data = pd.merge(nhts_data_t, nhts_data_h, on='HOUSEID', how='left')\n",
    "nhts_data = pd.merge(nhts_data, nhts_data_p, on=['HOUSEID', 'PERSONID'], how='left')\n",
    "\n",
    "# Give each set of daily trips a unique chain id (each will be an input to model)\n",
    "nhts_data['CHAINID'] = nhts_data.groupby(['TDAYDATE','HOUSEID','PERSONID']).ngroup().values\n",
    "nhts_data = nhts_data.drop(labels=['TDAYDATE','TDCASEID','HOUSEID','PERSONID'], axis=1)\n",
    "\n",
    "# Remove NA values and check n before/after\n",
    "print(f\"Dataset n={len(nhts_data)} pre-cleaning\")\n",
    "nan_indices = list((nhts_data < 0).any(axis=1))\n",
    "nan_ids = nhts_data[nan_indices][['CHAINID']].values.flatten()\n",
    "nhts_data = nhts_data[~(nhts_data['CHAINID'].isin(nan_ids))]\n",
    "print(f\"Dataset n={len(nhts_data)} post-cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only numeric variables, only dynamic variables\n",
    "nhts_data = nhts_data[['TRAVDAY','TDTRPNUM','STRTTIME','TRVLCMIN','TRPMILES','TRPTRANS','CHAINID']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAVDAY</th>\n",
       "      <th>TDTRPNUM</th>\n",
       "      <th>STRTTIME</th>\n",
       "      <th>TRVLCMIN</th>\n",
       "      <th>TRPMILES</th>\n",
       "      <th>TRPTRANS</th>\n",
       "      <th>CHAINID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>120</td>\n",
       "      <td>84.004</td>\n",
       "      <td>6</td>\n",
       "      <td>46938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1800</td>\n",
       "      <td>150</td>\n",
       "      <td>81.628</td>\n",
       "      <td>6</td>\n",
       "      <td>46938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1115</td>\n",
       "      <td>15</td>\n",
       "      <td>8.017</td>\n",
       "      <td>6</td>\n",
       "      <td>46940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2330</td>\n",
       "      <td>10</td>\n",
       "      <td>8.017</td>\n",
       "      <td>6</td>\n",
       "      <td>46940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>550</td>\n",
       "      <td>15</td>\n",
       "      <td>3.395</td>\n",
       "      <td>4</td>\n",
       "      <td>24626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923567</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>810</td>\n",
       "      <td>27</td>\n",
       "      <td>1.168</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923568</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1320</td>\n",
       "      <td>8</td>\n",
       "      <td>0.238</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923569</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1415</td>\n",
       "      <td>5</td>\n",
       "      <td>0.238</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923570</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1820</td>\n",
       "      <td>25</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923571</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1848</td>\n",
       "      <td>7</td>\n",
       "      <td>0.325</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405590 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TRAVDAY  TDTRPNUM  STRTTIME  TRVLCMIN  TRPMILES  TRPTRANS  CHAINID\n",
       "2             2         1       700       120    84.004         6    46938\n",
       "3             2         2      1800       150    81.628         6    46938\n",
       "6             5         1      1115        15     8.017         6    46940\n",
       "7             5         2      2330        10     8.017         6    46940\n",
       "8             5         1       550        15     3.395         4    24626\n",
       "...         ...       ...       ...       ...       ...       ...      ...\n",
       "923567        3         1       810        27     1.168         1    93638\n",
       "923568        3         2      1320         8     0.238         1    93638\n",
       "923569        3         3      1415         5     0.238         1    93638\n",
       "923570        3         4      1820        25     0.867         1    93638\n",
       "923571        3         5      1848         7     0.325         1    93638\n",
       "\n",
       "[405590 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data that will be fed into model\n",
    "model_data_df = nhts_data\n",
    "model_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23897\n",
      "3463\n",
      "378\n"
     ]
    }
   ],
   "source": [
    "# Remove chains that have more than 5 trips in them\n",
    "MAX_TIMESTEPS = 5\n",
    "long_chains = model_data_df[model_data_df['TDTRPNUM'] > MAX_TIMESTEPS][['CHAINID']].values.flatten()\n",
    "model_data_df = model_data_df[~model_data_df['CHAINID'].isin(long_chains)]\n",
    "print(len(pd.unique(long_chains)))\n",
    "\n",
    "## Remove outliers\n",
    "# Filter model data into train/test data\n",
    "chain_ids = pd.unique(model_data_df['CHAINID'])\n",
    "train_idx = round(len(chain_ids)*.9)\n",
    "train_ids = chain_ids[0:train_idx]\n",
    "test_ids = chain_ids[train_idx:len(chain_ids)]\n",
    "train_data_df = model_data_df[model_data_df['CHAINID'].isin(train_ids)]\n",
    "test_data_df = model_data_df[model_data_df['CHAINID'].isin(test_ids)]\n",
    "\n",
    "# Standardize the input data from -1 to 1 for numerical variables, remove outliers (x > 3 SD)\n",
    "scaler_train = skpre.StandardScaler()\n",
    "scaler_test = skpre.StandardScaler()\n",
    "train_data = scaler_train.fit_transform(train_data_df.values)\n",
    "test_data = scaler_test.fit_transform(test_data_df.values)\n",
    "\n",
    "# Remove outliers from dataset once for training data...\n",
    "outlier_indices = np.where(np.any(train_data > 3, axis=1))[0]\n",
    "outlier_chains = train_data_df.iloc[outlier_indices,:][['CHAINID']].values.flatten()\n",
    "train_data_df = train_data_df[~train_data_df['CHAINID'].isin(outlier_chains)]\n",
    "print(len(outlier_chains))\n",
    "\n",
    "# ...and again for testing data (keep the scalers separate)\n",
    "outlier_indices = np.where(np.any(test_data > 3, axis=1))[0]\n",
    "outlier_chains = test_data_df.iloc[outlier_indices,:][['CHAINID']].values.flatten()\n",
    "test_data_df = test_data_df[~test_data_df['CHAINID'].isin(outlier_chains)]\n",
    "print(len(outlier_chains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the model data to a format that is usable by tensorflow: shape = (#samples, timestep size, #features)\n",
    "# TODO: Make sure this is working correctly\n",
    "train_samples = []\n",
    "chain_ids = pd.unique(train_data_df['CHAINID'])\n",
    "\n",
    "# This could be faster\n",
    "for chain_id in chain_ids:\n",
    "    data = train_data_df[train_data_df['CHAINID'] == chain_id].values\n",
    "    data = scaler_train.fit_transform(data).transpose()\n",
    "    data = keras.preprocessing.sequence.pad_sequences(data, MAX_TIMESTEPS, padding='post').transpose()\n",
    "    train_samples.append(data)\n",
    "\n",
    "# (samples, timesteps, features)\n",
    "train_data = np.array(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the model data to a format that is usable by tensorflow\n",
    "test_samples = []\n",
    "chain_ids = pd.unique(test_data_df['CHAINID'])\n",
    "\n",
    "# This could be faster\n",
    "for chain_id in chain_ids:\n",
    "    data = test_data_df[test_data_df['CHAINID'] == chain_id].values\n",
    "    data = scaler_test.fit_transform(data).transpose()\n",
    "    data = keras.preprocessing.sequence.pad_sequences(data, MAX_TIMESTEPS, padding='post').transpose()\n",
    "    test_samples.append(data)\n",
    "\n",
    "# (samples, timesteps, features)\n",
    "test_data = np.array(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61963, 5, 7)\n",
      "(6904, 5, 7)\n"
     ]
    }
   ],
   "source": [
    "# shape = (#samples, timestep size, #features)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters and Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep is a trip\n",
    "# Output of each timestep is departure time, duration, distance, mode, and purpose (dynamic)\n",
    "# Input of each timestep is person/hh variables, day of week, and previous timestep info (static + dynamic)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 5\n",
    "LEARN_RATE = 0.01\n",
    "RHO = 0.9\n",
    "MANIFEST_DIM = train_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 5, 7)]            0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 5, 10)             80        \n",
      "_________________________________________________________________\n",
      "lstm_48 (LSTM)               (None, 5, 10)             840       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 5, 7)              77        \n",
      "=================================================================\n",
      "Total params: 997\n",
      "Trainable params: 997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM layer requires inputs to be 3D tensor with shape [batch, timesteps, feature]\n",
    "inputs = keras.Input(shape=(MAX_TIMESTEPS, MANIFEST_DIM))\n",
    "dense = layers.Dense(10)(inputs)\n",
    "lstm = layers.LSTM(10, return_sequences=True)(dense)\n",
    "outputs = layers.Dense(MANIFEST_DIM)(lstm)\n",
    "\n",
    "# Define and print model\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6197/6197 [==============================] - 28s 4ms/step - loss: 0.0015\n",
      "Epoch 2/5\n",
      "6197/6197 [==============================] - 26s 4ms/step - loss: 4.1532e-04\n",
      "Epoch 3/5\n",
      "6197/6197 [==============================] - 25s 4ms/step - loss: 3.0878e-04\n",
      "Epoch 4/5\n",
      "6197/6197 [==============================] - 26s 4ms/step - loss: 2.7703e-04\n",
      "Epoch 5/5\n",
      "6197/6197 [==============================] - 27s 4ms/step - loss: 2.5291e-04\n"
     ]
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.RMSprop(learning_rate=LEARN_RATE, rho=RHO))\n",
    "history = model.fit(train_data, train_data, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model loss/training progress\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Training History\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"total\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent vars from the encoder; feed to decoder and get sampled manifest variables\n",
    "z_mean, z_logvar, z = vae.encoder.predict(test_data[:,:MANIFEST_DIM])\n",
    "\n",
    "# Record the posterior trained distributions for z\n",
    "latent_means = []\n",
    "latent_vars = []\n",
    "\n",
    "# Determine the average values for the mean/logvariance of the latent variables\n",
    "for i in range(0, LATENT_DIM):\n",
    "    epsilon = np.random.normal(loc=0, scale=1, size=1000)\n",
    "    avg_mean = np.mean(z_mean[:,i])\n",
    "    latent_means.append(avg_mean)\n",
    "    avg_var = np.exp(np.mean(z_logvar[:,i]))\n",
    "    latent_vars.append(avg_var)\n",
    "    print(f\"Latent Variable: {i}\")\n",
    "    print(f\"Mean: {avg_mean}\")\n",
    "    print(f\"Variance: {np.exp(avg_var)}\\n\")\n",
    "    samples = avg_mean + (avg_var * epsilon)\n",
    "    plt.hist(samples, bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw predictions from test data\n",
    "results = vae.predict(test_data)\n",
    "loss_num, loss_cat = get_reconstruction_loss(test_data, results, CAT_IDX, CAT_LENGTHS)\n",
    "print(f\"Numerical Variable Loss: {loss_num}\")\n",
    "print(f\"Categorical Variable Loss: {loss_cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numeric results back to real variable values\n",
    "results_num = scaler_pers.inverse_transform(results[0])\n",
    "results_df = pd.DataFrame(results_num)\n",
    "\n",
    "# Transform categorical results back to real variable values\n",
    "for x in results[1]:\n",
    "    result = np.argmax(x, axis=1) + 1\n",
    "    results_df[f\"{x}\"] = result\n",
    "\n",
    "# Add back original variables names to the results\n",
    "results_df.columns = VAR_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numeric test data back to real variable values\n",
    "test_data_num = scaler_pers.inverse_transform(test_data[:,:CAT_IDX])\n",
    "test_data_df = pd.DataFrame(test_data_num)\n",
    "\n",
    "# Transform categorical test data back to real variable values\n",
    "current = CAT_IDX\n",
    "for x in CAT_LENGTHS:\n",
    "    test_data_cat = test_data[:,current:(current + x)]\n",
    "    test_data_cat = np.argmax(test_data_cat, axis=1) + 1\n",
    "    test_data_df[f\"{x}\"] = test_data_cat\n",
    "    current += x\n",
    "\n",
    "# Add back original variables names to the test data\n",
    "test_data_df.columns = VAR_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting numerical variables\n",
    "for col_idx in range(0, CAT_IDX):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "\n",
    "    plt.hist(results_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(test_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting categorical variables\n",
    "for col_idx in range(CAT_IDX, VAR_DIM):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "    \n",
    "    plt.hist(results_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(test_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list of households from the test data (in future can be generated with separate vae)\n",
    "x = pd.DataFrame(test_data[:,MANIFEST_DIM:]).reset_index(drop=True) # Scaled hh_input values\n",
    "y = model_data_df.iloc[train_idx:][['NP']].reset_index(drop=True) # Unscaled number of persons value\n",
    "z = pd.concat([x,y],axis=1)\n",
    "z.columns = ['HINCP','NP','VEH','SIZE']\n",
    "\n",
    "# Multiply the inputs by the number of persons per household (hh of size 3 becomes 3 rows with same scaled hh inputs)\n",
    "z = z.reindex(z.index.repeat(z['SIZE']))\n",
    "z = z[['HINCP','NP','VEH']].values\n",
    "\n",
    "# Generate random normal sample to represent each latent variable, for each row (different person per row)\n",
    "epsilon = np.random.normal(loc=0, scale=1, size=(len(z), LATENT_DIM))\n",
    "inputs = np.concatenate((epsilon, z), axis=-1)\n",
    "\n",
    "# Generate persons; each person has unique latent input, plus shared hh inputs with their household\n",
    "results = vae.decoder.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numeric results back to real variable values\n",
    "results_num = scaler_pers.inverse_transform(results[0])\n",
    "results_df = pd.DataFrame(results_num)\n",
    "\n",
    "# Transform categorical results back to real variable values\n",
    "for x in results[1]:\n",
    "    result = np.argmax(x, axis=1) + 1\n",
    "    results_df[f\"{x}\"] = result\n",
    "\n",
    "# Add back original variables names to the results\n",
    "results_df.columns = VAR_NAMES\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting numerical variables\n",
    "for col_idx in range(0, CAT_IDX):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "\n",
    "    plt.hist(results_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(test_data_plt, bins=100)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distributions of the resulting categorical variables\n",
    "for col_idx in range(CAT_IDX, VAR_DIM):\n",
    "    results_data_plt = results_df.iloc[:,col_idx]\n",
    "    test_data_plt = test_data_df.iloc[:,col_idx]\n",
    "    \n",
    "    plt.hist(results_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - reconstructed distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(test_data_plt)\n",
    "    plt.xlim(min(test_data_plt),max(test_data_plt))\n",
    "    plt.title(f\"var {col_idx} - prior distribution\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
