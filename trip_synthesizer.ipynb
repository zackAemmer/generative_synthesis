{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "###### Build up fundamental model:\n",
    "-Test speeds vs traditional methods  \n",
    "-Energy distance test for distributions  \n",
    "-Check distributions of variables within households, compare to naive method w/borysov model somehow  \n",
    "-Test making epsilon the same distribution as the actual posteriors in the model\n",
    "\n",
    "###### Differentiate from the GenSynth paper:\n",
    "-Travel diaries  \n",
    "-Method/heuristic/rules for checking large number of attributes  \n",
    "-New models; Disentangled VAE/GAN  \n",
    "-Model population changes over time RNN  \n",
    "-Behavioral variables  \n",
    "\n",
    "###### They suggest:\n",
    "-Incorporate RNN to generate trip chains (time, location, mode, purpose)  \n",
    "-Use GAN/other method to generate less inconsistencies  \n",
    "-Address next stage of re-sampling to get future populations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input to training the model is a person's daily trip diary\n",
    "# Inputs; day of week, characteristics of person/hh\n",
    "# Outputs; trip purpose, mode, duration, distance\n",
    "# How to include Time of Day?\n",
    "# Timesteps could either be hours in the day, or trips in a chain?\n",
    "    # If a timestep is a trip, add the time of departure to the output variables\n",
    "    \n",
    "# Timestep is a trip\n",
    "# Output of each timestep is departure time, duration, distance, mode, and purpose (y)\n",
    "# Input of each timestep is person/hh variables, day of week, and previous timestep info (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skpre\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the persons PUMS dataset for WA state\n",
    "t_df = pd.read_csv('data/NHTS/nhts_survey/trippub.csv')\n",
    "p_df = pd.read_csv('data/NHTS/nhts_survey/perpub.csv')\n",
    "h_df = pd.read_csv('data/NHTS/nhts_survey/hhpub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Variables and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset n=923572 pre-cleaning\n",
      "Dataset n=405590 post-cleaning\n"
     ]
    }
   ],
   "source": [
    "# Filter to desired variables (numeric then categorical)\n",
    "#TRIPPURP = simplified why/from\n",
    "nhts_data_t = t_df[['TDCASEID','HOUSEID','PERSONID','TDAYDATE','TRAVDAY','TDTRPNUM','STRTTIME','TRVLCMIN','TRPMILES','TRPTRANS','WHYFROM','WHYTO']]\n",
    "nhts_data_p = p_df[['HOUSEID','PERSONID','R_AGE','TIMETOWK','EDUC','R_SEX','OCCAT']]\n",
    "nhts_data_h = h_df[['HOUSEID','HHSIZE','HHFAMINC','HHVEHCNT']]\n",
    "del t_df\n",
    "del p_df\n",
    "del h_df\n",
    "nhts_data = pd.merge(nhts_data_t, nhts_data_h, on='HOUSEID', how='left')\n",
    "nhts_data = pd.merge(nhts_data, nhts_data_p, on=['HOUSEID', 'PERSONID'], how='left')\n",
    "\n",
    "# Give each set of daily trips a unique chain id (each will be an input to model)\n",
    "nhts_data['CHAINID'] = nhts_data.groupby(['TDAYDATE','HOUSEID','PERSONID']).ngroup().values\n",
    "nhts_data = nhts_data.drop(labels=['TDAYDATE','TDCASEID','HOUSEID','PERSONID'], axis=1)\n",
    "\n",
    "# Remove NA values and check n before/after\n",
    "print(f\"Dataset n={len(nhts_data)} pre-cleaning\")\n",
    "nan_indices = list((nhts_data < 0).any(axis=1))\n",
    "nan_ids = nhts_data[nan_indices][['CHAINID']].values.flatten()\n",
    "nhts_data = nhts_data[~(nhts_data['CHAINID'].isin(nan_ids))]\n",
    "print(f\"Dataset n={len(nhts_data)} post-cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only numeric variables, only dynamic variables\n",
    "nhts_data = nhts_data[['TRAVDAY','TDTRPNUM','STRTTIME','TRVLCMIN','TRPMILES','TRPTRANS','CHAINID']].copy()\n",
    "MANIFEST_DIM = nhts_data.values.shape[1]-1 # All except chainid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAVDAY</th>\n",
       "      <th>TDTRPNUM</th>\n",
       "      <th>STRTTIME</th>\n",
       "      <th>TRVLCMIN</th>\n",
       "      <th>TRPMILES</th>\n",
       "      <th>TRPTRANS</th>\n",
       "      <th>CHAINID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>120</td>\n",
       "      <td>84.004</td>\n",
       "      <td>6</td>\n",
       "      <td>46938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1800</td>\n",
       "      <td>150</td>\n",
       "      <td>81.628</td>\n",
       "      <td>6</td>\n",
       "      <td>46938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1115</td>\n",
       "      <td>15</td>\n",
       "      <td>8.017</td>\n",
       "      <td>6</td>\n",
       "      <td>46940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2330</td>\n",
       "      <td>10</td>\n",
       "      <td>8.017</td>\n",
       "      <td>6</td>\n",
       "      <td>46940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>550</td>\n",
       "      <td>15</td>\n",
       "      <td>3.395</td>\n",
       "      <td>4</td>\n",
       "      <td>24626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923567</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>810</td>\n",
       "      <td>27</td>\n",
       "      <td>1.168</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923568</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1320</td>\n",
       "      <td>8</td>\n",
       "      <td>0.238</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923569</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1415</td>\n",
       "      <td>5</td>\n",
       "      <td>0.238</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923570</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1820</td>\n",
       "      <td>25</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923571</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1848</td>\n",
       "      <td>7</td>\n",
       "      <td>0.325</td>\n",
       "      <td>1</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405590 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TRAVDAY  TDTRPNUM  STRTTIME  TRVLCMIN  TRPMILES  TRPTRANS  CHAINID\n",
       "2             2         1       700       120    84.004         6    46938\n",
       "3             2         2      1800       150    81.628         6    46938\n",
       "6             5         1      1115        15     8.017         6    46940\n",
       "7             5         2      2330        10     8.017         6    46940\n",
       "8             5         1       550        15     3.395         4    24626\n",
       "...         ...       ...       ...       ...       ...       ...      ...\n",
       "923567        3         1       810        27     1.168         1    93638\n",
       "923568        3         2      1320         8     0.238         1    93638\n",
       "923569        3         3      1415         5     0.238         1    93638\n",
       "923570        3         4      1820        25     0.867         1    93638\n",
       "923571        3         5      1848         7     0.325         1    93638\n",
       "\n",
       "[405590 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data that will be fed into model\n",
    "model_data_df = nhts_data\n",
    "model_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15040\n",
      "4448\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "# Remove chains that have more than 5 trips in them\n",
    "MAX_TIMESTEPS = 5  # Make sure to add 1 when generating the chain data (1 gets removed in training)\n",
    "long_chains = model_data_df[model_data_df['TDTRPNUM'] > MAX_TIMESTEPS+1][['CHAINID']].values.flatten()\n",
    "model_data_df = model_data_df[~model_data_df['CHAINID'].isin(long_chains)]\n",
    "print(len(pd.unique(long_chains)))\n",
    "\n",
    "## Remove outliers\n",
    "# Filter model data into train/test data\n",
    "chain_ids = pd.unique(model_data_df['CHAINID'])\n",
    "train_idx = round(len(chain_ids)*.9)\n",
    "train_ids = chain_ids[0:train_idx]\n",
    "test_ids = chain_ids[train_idx:len(chain_ids)]\n",
    "train_data_df = model_data_df[model_data_df['CHAINID'].isin(train_ids)]\n",
    "test_data_df = model_data_df[model_data_df['CHAINID'].isin(test_ids)]\n",
    "\n",
    "# Standardize the input data from -1 to 1 for numerical variables, remove outliers (x > 3 SD)\n",
    "scaler_train = skpre.StandardScaler()\n",
    "scaler_test = skpre.StandardScaler()\n",
    "train_data = scaler_train.fit_transform(train_data_df.values)\n",
    "test_data = scaler_test.fit_transform(test_data_df.values)\n",
    "\n",
    "# Remove outliers from dataset once for training data...\n",
    "outlier_indices = np.where(np.any(train_data > 3, axis=1))[0]\n",
    "outlier_chains = train_data_df.iloc[outlier_indices,:][['CHAINID']].values.flatten()\n",
    "train_data_df = train_data_df[~train_data_df['CHAINID'].isin(outlier_chains)]\n",
    "print(len(outlier_chains))\n",
    "\n",
    "# ...and again for testing data (keep the scalers separate)\n",
    "outlier_indices = np.where(np.any(test_data > 3, axis=1))[0]\n",
    "outlier_chains = test_data_df.iloc[outlier_indices,:][['CHAINID']].values.flatten()\n",
    "test_data_df = test_data_df[~test_data_df['CHAINID'].isin(outlier_chains)]\n",
    "print(len(outlier_chains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the model data to a format that is usable by tensorflow: shape = (#samples, timestep size, #features)\n",
    "train_samples_x = []\n",
    "train_samples_y = []\n",
    "chain_ids = pd.unique(train_data_df['CHAINID'])\n",
    "train_data_df.iloc[:,:MANIFEST_DIM] = scaler_train.fit_transform(train_data_df.iloc[:,:MANIFEST_DIM].values)  # Scale all variables except Chainid\n",
    "\n",
    "# This could be faster\n",
    "for chain_id in chain_ids:\n",
    "    data = train_data_df[train_data_df['CHAINID'] == chain_id].values.transpose()\n",
    "    data = keras.preprocessing.sequence.pad_sequences(data, MAX_TIMESTEPS+1, padding='pre').transpose()\n",
    "    data = data[:,:-1]  # Remove Chainid\n",
    "    data_offset = data[1:,:]  # Validation data is offset by 1 timestep\n",
    "    data = data[:-1,:]  # Remove the final timestep from input data (no validation for it)\n",
    "    train_samples_x.append(data)\n",
    "    train_samples_y.append(data_offset)\n",
    "\n",
    "# Dimensions are: (samples, timesteps, features)\n",
    "train_data_x = np.array(train_samples_x)\n",
    "train_data_y = np.array(train_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the model data to a format that is usable by tensorflow: shape = (#samples, timestep size, #features)\n",
    "# TODO: Make sure this is working correctly\n",
    "test_samples_x = []\n",
    "test_samples_y = []\n",
    "chain_ids = pd.unique(test_data_df['CHAINID'])\n",
    "test_data_df.iloc[:,:MANIFEST_DIM] = scaler_test.fit_transform(test_data_df.iloc[:,:MANIFEST_DIM].values)  # Scale all variables except Chainid\n",
    "\n",
    "# This could be faster\n",
    "for chain_id in chain_ids:\n",
    "    data = test_data_df[test_data_df['CHAINID'] == chain_id].values.transpose()\n",
    "    data = keras.preprocessing.sequence.pad_sequences(data, MAX_TIMESTEPS+1, padding='pre').transpose()\n",
    "    data = data[:,:-1]  # Remove Chainid\n",
    "    data_offset = data[1:,:]  # Validation data is offset by 1 timestep\n",
    "    data = data[:-1,:]  # Remove the final timestep from input data (no validation for it)\n",
    "    test_samples_x.append(data)\n",
    "    test_samples_y.append(data_offset)\n",
    "\n",
    "# Dimensions are: (samples, timesteps, features)\n",
    "test_data_x = np.array(test_samples_x)\n",
    "test_data_y = np.array(test_samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69288, 5, 6)\n",
      "(69288, 5, 6)\n",
      "(7733, 5, 6)\n",
      "(7733, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "# shape = (#samples, timestep size, #features)\n",
    "print(train_data_x.shape)\n",
    "print(train_data_y.shape)\n",
    "print(test_data_x.shape)\n",
    "print(test_data_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters and Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep is a trip\n",
    "# Output of each timestep is departure time, duration, distance, mode, and purpose (dynamic)\n",
    "# Input of each timestep is person/hh variables, day of week, and previous timestep info (static + dynamic)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5\n",
    "LEARN_RATE = 0.01\n",
    "RHO = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5, 6)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5, 10)             70        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 5, 10)             840       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5, 6)              66        \n",
      "=================================================================\n",
      "Total params: 976\n",
      "Trainable params: 976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# I should try embedding trips as a classification => one for each mode, orig/dest purpose\n",
    "# How to do dist/time/other? Separate variables?\n",
    "\n",
    "# LSTM layer requires inputs to be 3D tensor with shape [batch, timesteps, feature]\n",
    "inputs = keras.Input(shape=(MAX_TIMESTEPS, MANIFEST_DIM))\n",
    "dense = layers.Dense(10, activation=\"tanh\")(inputs)\n",
    "lstm = layers.LSTM(10, activation=\"tanh\", return_sequences=True)(dense)\n",
    "outputs = layers.Dense(MANIFEST_DIM, activation=\"tanh\")(lstm)\n",
    "\n",
    "# Define and print model\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./model_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "693/693 [==============================] - ETA: 0s - loss: 0.2345\n",
      "Epoch 00001: loss improved from inf to 0.23453, saving model to ./model_checkpoints/weights-improvement-01-0.2345.hdf5\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.2345\n",
      "Epoch 2/5\n",
      "687/693 [============================>.] - ETA: 0s - loss: 0.2244\n",
      "Epoch 00002: loss improved from 0.23453 to 0.22450, saving model to ./model_checkpoints/weights-improvement-02-0.2245.hdf5\n",
      "693/693 [==============================] - 3s 5ms/step - loss: 0.2245\n",
      "Epoch 3/5\n",
      "686/693 [============================>.] - ETA: 0s - loss: 0.2232\n",
      "Epoch 00003: loss improved from 0.22450 to 0.22333, saving model to ./model_checkpoints/weights-improvement-03-0.2233.hdf5\n",
      "693/693 [==============================] - 3s 5ms/step - loss: 0.2233\n",
      "Epoch 4/5\n",
      "689/693 [============================>.] - ETA: 0s - loss: 0.2225\n",
      "Epoch 00004: loss improved from 0.22333 to 0.22264, saving model to ./model_checkpoints/weights-improvement-04-0.2226.hdf5\n",
      "693/693 [==============================] - 3s 5ms/step - loss: 0.2226\n",
      "Epoch 5/5\n",
      "689/693 [============================>.] - ETA: 0s - loss: 0.2220\n",
      "Epoch 00005: loss improved from 0.22264 to 0.22210, saving model to ./model_checkpoints/weights-improvement-05-0.2221.hdf5\n",
      "693/693 [==============================] - 3s 4ms/step - loss: 0.2221\n"
     ]
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.RMSprop(learning_rate=LEARN_RATE, rho=RHO))\n",
    "history = model.fit(train_data_x, train_data_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model loss/training progress\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Training History\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"total\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the best result of the training\n",
    "filename = \"./model_checkpoints/weights-improvement-05-0.2219.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.RMSprop(learning_rate=LEARN_RATE, rho=RHO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 trip chains, each with MAXTIMESTEPS timesteps (each represents 1 day for 1 individual)\n",
    "NUM_CHAINS = 10\n",
    "result_chains = []\n",
    "\n",
    "for i in range(NUM_CHAINS):\n",
    "    seed = np.random.rand(MAX_TIMESTEPS, MANIFEST_DIM)\n",
    "    #seed = np.zeros((MAX_TIMESTEPS, MANIFEST_DIM))  # Generate either random, or zero-based seed for first trip\n",
    "    seed = np.reshape(seed, (1, MAX_TIMESTEPS, MANIFEST_DIM))\n",
    "\n",
    "    # Make first prediction to start from\n",
    "    prediction = model.predict(seed, verbose=0)\n",
    "    result = prediction\n",
    "\n",
    "    # After each prediction, add the prediction to the results and re-predict the remaining timesteps from the results\n",
    "    for j in range(1, MAX_TIMESTEPS):\n",
    "        prediction = model.predict(result, verbose=0)\n",
    "        result[:,j:,:] = prediction[:,j:,:]\n",
    "    result_chains.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data in the same dimensions as the input data\n",
    "synthetic_chains = np.array(result_chains).reshape(NUM_CHAINS, MAX_TIMESTEPS, MANIFEST_DIM)\n",
    "\n",
    "# Assign a trip chain id to each synthetic chain\n",
    "chain_ids = []\n",
    "# Get np array of chain ids to append, must be same dims as the synthetic results\n",
    "for i in range(NUM_CHAINS):\n",
    "    to_add = np.zeros(MAX_TIMESTEPS) + i\n",
    "    to_add = np.reshape(to_add, (MAX_TIMESTEPS, 1))\n",
    "    chain_ids.append(to_add)\n",
    "chain_ids = np.array(chain_ids)\n",
    "synthetic_chains = np.append(synthetic_chains, chain_ids, axis=2).reshape((NUM_CHAINS*MAX_TIMESTEPS), MANIFEST_DIM+1)\n",
    "\n",
    "# Scale the results back to real values\n",
    "synthetic_chains[:,:MANIFEST_DIM] = scaler_train.inverse_transform(synthetic_chains[:,:MANIFEST_DIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.084425</td>\n",
       "      <td>3.802717</td>\n",
       "      <td>1293.232305</td>\n",
       "      <td>29.740421</td>\n",
       "      <td>19.199403</td>\n",
       "      <td>3.842852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.029311</td>\n",
       "      <td>3.866697</td>\n",
       "      <td>1506.713716</td>\n",
       "      <td>27.962944</td>\n",
       "      <td>20.062413</td>\n",
       "      <td>3.548308</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.714193</td>\n",
       "      <td>3.878080</td>\n",
       "      <td>1784.384838</td>\n",
       "      <td>19.341918</td>\n",
       "      <td>13.951094</td>\n",
       "      <td>3.048087</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.261178</td>\n",
       "      <td>3.883575</td>\n",
       "      <td>1797.549314</td>\n",
       "      <td>15.599817</td>\n",
       "      <td>8.311004</td>\n",
       "      <td>3.206095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.745648</td>\n",
       "      <td>3.887717</td>\n",
       "      <td>1796.622665</td>\n",
       "      <td>14.020347</td>\n",
       "      <td>5.197116</td>\n",
       "      <td>3.263625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.170881</td>\n",
       "      <td>2.536503</td>\n",
       "      <td>1615.570632</td>\n",
       "      <td>22.266912</td>\n",
       "      <td>10.304450</td>\n",
       "      <td>5.350487</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.171688</td>\n",
       "      <td>3.692662</td>\n",
       "      <td>1500.644337</td>\n",
       "      <td>19.560275</td>\n",
       "      <td>6.533405</td>\n",
       "      <td>5.585594</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.274171</td>\n",
       "      <td>3.884380</td>\n",
       "      <td>1713.973949</td>\n",
       "      <td>18.219146</td>\n",
       "      <td>4.113285</td>\n",
       "      <td>5.730975</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.386611</td>\n",
       "      <td>3.890295</td>\n",
       "      <td>1793.920032</td>\n",
       "      <td>37.061753</td>\n",
       "      <td>21.421566</td>\n",
       "      <td>4.719471</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.746015</td>\n",
       "      <td>3.890763</td>\n",
       "      <td>1796.553582</td>\n",
       "      <td>29.500502</td>\n",
       "      <td>19.236215</td>\n",
       "      <td>4.750329</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.466423</td>\n",
       "      <td>3.810704</td>\n",
       "      <td>1466.874357</td>\n",
       "      <td>32.690355</td>\n",
       "      <td>20.672849</td>\n",
       "      <td>3.931757</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.106173</td>\n",
       "      <td>3.867734</td>\n",
       "      <td>1575.923374</td>\n",
       "      <td>26.707565</td>\n",
       "      <td>19.672221</td>\n",
       "      <td>3.646650</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.919363</td>\n",
       "      <td>3.877104</td>\n",
       "      <td>1790.886503</td>\n",
       "      <td>19.093613</td>\n",
       "      <td>13.706893</td>\n",
       "      <td>3.200008</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.869127</td>\n",
       "      <td>3.883059</td>\n",
       "      <td>1798.879986</td>\n",
       "      <td>15.557579</td>\n",
       "      <td>9.145337</td>\n",
       "      <td>3.336426</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.581195</td>\n",
       "      <td>3.886863</td>\n",
       "      <td>1798.610998</td>\n",
       "      <td>13.296674</td>\n",
       "      <td>5.122201</td>\n",
       "      <td>3.472623</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.158961</td>\n",
       "      <td>3.571101</td>\n",
       "      <td>1684.204426</td>\n",
       "      <td>24.509088</td>\n",
       "      <td>14.637121</td>\n",
       "      <td>5.503001</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.725500</td>\n",
       "      <td>3.879816</td>\n",
       "      <td>1718.812138</td>\n",
       "      <td>25.946059</td>\n",
       "      <td>17.410468</td>\n",
       "      <td>5.270173</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.552518</td>\n",
       "      <td>3.887147</td>\n",
       "      <td>1733.439758</td>\n",
       "      <td>19.313818</td>\n",
       "      <td>13.009885</td>\n",
       "      <td>4.748834</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.421236</td>\n",
       "      <td>3.888974</td>\n",
       "      <td>1799.752302</td>\n",
       "      <td>14.996562</td>\n",
       "      <td>7.869254</td>\n",
       "      <td>4.218462</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.309075</td>\n",
       "      <td>3.889969</td>\n",
       "      <td>1789.924771</td>\n",
       "      <td>18.096969</td>\n",
       "      <td>12.767435</td>\n",
       "      <td>4.183788</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.194257</td>\n",
       "      <td>3.678310</td>\n",
       "      <td>1647.538984</td>\n",
       "      <td>32.464651</td>\n",
       "      <td>20.549492</td>\n",
       "      <td>4.819023</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.468170</td>\n",
       "      <td>3.854944</td>\n",
       "      <td>1672.027054</td>\n",
       "      <td>24.103537</td>\n",
       "      <td>18.933053</td>\n",
       "      <td>4.117983</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.164059</td>\n",
       "      <td>3.881997</td>\n",
       "      <td>1791.354796</td>\n",
       "      <td>19.461623</td>\n",
       "      <td>14.317987</td>\n",
       "      <td>3.594925</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.116176</td>\n",
       "      <td>3.885858</td>\n",
       "      <td>1799.326410</td>\n",
       "      <td>14.230771</td>\n",
       "      <td>7.399906</td>\n",
       "      <td>3.622363</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.188899</td>\n",
       "      <td>3.887815</td>\n",
       "      <td>1798.977347</td>\n",
       "      <td>13.925725</td>\n",
       "      <td>6.970663</td>\n",
       "      <td>3.759375</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.919832</td>\n",
       "      <td>3.766389</td>\n",
       "      <td>1676.027498</td>\n",
       "      <td>25.793602</td>\n",
       "      <td>14.995999</td>\n",
       "      <td>5.385042</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.810830</td>\n",
       "      <td>3.875770</td>\n",
       "      <td>1472.906902</td>\n",
       "      <td>21.369425</td>\n",
       "      <td>14.858473</td>\n",
       "      <td>5.396352</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.425331</td>\n",
       "      <td>3.887002</td>\n",
       "      <td>1758.440117</td>\n",
       "      <td>25.071947</td>\n",
       "      <td>17.726569</td>\n",
       "      <td>4.502183</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.377883</td>\n",
       "      <td>3.888670</td>\n",
       "      <td>1798.792962</td>\n",
       "      <td>16.123828</td>\n",
       "      <td>9.661445</td>\n",
       "      <td>3.901230</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.351032</td>\n",
       "      <td>3.889489</td>\n",
       "      <td>1800.632763</td>\n",
       "      <td>13.811703</td>\n",
       "      <td>6.990955</td>\n",
       "      <td>3.840350</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.465528</td>\n",
       "      <td>3.804899</td>\n",
       "      <td>1321.393377</td>\n",
       "      <td>26.113540</td>\n",
       "      <td>15.097145</td>\n",
       "      <td>5.465224</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.467101</td>\n",
       "      <td>3.875893</td>\n",
       "      <td>1498.503158</td>\n",
       "      <td>24.173282</td>\n",
       "      <td>16.311282</td>\n",
       "      <td>5.031290</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.279537</td>\n",
       "      <td>3.885512</td>\n",
       "      <td>1760.515531</td>\n",
       "      <td>20.644871</td>\n",
       "      <td>14.062718</td>\n",
       "      <td>4.065789</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.359911</td>\n",
       "      <td>3.887612</td>\n",
       "      <td>1797.532257</td>\n",
       "      <td>15.220594</td>\n",
       "      <td>7.887485</td>\n",
       "      <td>3.872348</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.308794</td>\n",
       "      <td>3.888934</td>\n",
       "      <td>1798.677690</td>\n",
       "      <td>13.170182</td>\n",
       "      <td>5.395205</td>\n",
       "      <td>3.879461</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.144594</td>\n",
       "      <td>3.679558</td>\n",
       "      <td>1478.121364</td>\n",
       "      <td>34.118477</td>\n",
       "      <td>20.720334</td>\n",
       "      <td>5.314880</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.146727</td>\n",
       "      <td>2.342519</td>\n",
       "      <td>1687.876822</td>\n",
       "      <td>23.157004</td>\n",
       "      <td>19.084775</td>\n",
       "      <td>4.636912</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.249733</td>\n",
       "      <td>3.241539</td>\n",
       "      <td>1268.168002</td>\n",
       "      <td>25.596265</td>\n",
       "      <td>19.470906</td>\n",
       "      <td>4.116395</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.221779</td>\n",
       "      <td>3.850586</td>\n",
       "      <td>1612.037400</td>\n",
       "      <td>21.496117</td>\n",
       "      <td>17.036536</td>\n",
       "      <td>3.705089</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.926059</td>\n",
       "      <td>3.880613</td>\n",
       "      <td>1782.266255</td>\n",
       "      <td>18.360120</td>\n",
       "      <td>12.254652</td>\n",
       "      <td>3.634124</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.479139</td>\n",
       "      <td>2.356116</td>\n",
       "      <td>1605.777898</td>\n",
       "      <td>33.256154</td>\n",
       "      <td>20.417833</td>\n",
       "      <td>4.522511</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5.563971</td>\n",
       "      <td>2.716440</td>\n",
       "      <td>1449.737472</td>\n",
       "      <td>31.228890</td>\n",
       "      <td>19.960433</td>\n",
       "      <td>4.383212</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5.716483</td>\n",
       "      <td>3.882619</td>\n",
       "      <td>1598.946421</td>\n",
       "      <td>25.510939</td>\n",
       "      <td>16.877881</td>\n",
       "      <td>4.805733</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.692486</td>\n",
       "      <td>3.890131</td>\n",
       "      <td>1784.283320</td>\n",
       "      <td>22.409438</td>\n",
       "      <td>13.657009</td>\n",
       "      <td>5.028234</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.629410</td>\n",
       "      <td>3.890950</td>\n",
       "      <td>1800.954830</td>\n",
       "      <td>18.187672</td>\n",
       "      <td>9.569073</td>\n",
       "      <td>4.098025</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.462962</td>\n",
       "      <td>3.799878</td>\n",
       "      <td>1369.781138</td>\n",
       "      <td>32.361093</td>\n",
       "      <td>20.411359</td>\n",
       "      <td>4.967579</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.196876</td>\n",
       "      <td>3.868982</td>\n",
       "      <td>1730.332760</td>\n",
       "      <td>29.434293</td>\n",
       "      <td>20.171021</td>\n",
       "      <td>3.881857</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.080861</td>\n",
       "      <td>3.880762</td>\n",
       "      <td>1789.269900</td>\n",
       "      <td>19.269377</td>\n",
       "      <td>14.249326</td>\n",
       "      <td>3.500343</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.052663</td>\n",
       "      <td>3.884902</td>\n",
       "      <td>1798.900774</td>\n",
       "      <td>13.704530</td>\n",
       "      <td>6.167322</td>\n",
       "      <td>3.622772</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.068037</td>\n",
       "      <td>3.887472</td>\n",
       "      <td>1798.061547</td>\n",
       "      <td>13.229344</td>\n",
       "      <td>5.492635</td>\n",
       "      <td>3.763575</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1            2          3          4         5    6\n",
       "0   4.084425  3.802717  1293.232305  29.740421  19.199403  3.842852  0.0\n",
       "1   4.029311  3.866697  1506.713716  27.962944  20.062413  3.548308  0.0\n",
       "2   3.714193  3.878080  1784.384838  19.341918  13.951094  3.048087  0.0\n",
       "3   3.261178  3.883575  1797.549314  15.599817   8.311004  3.206095  0.0\n",
       "4   2.745648  3.887717  1796.622665  14.020347   5.197116  3.263625  0.0\n",
       "5   5.170881  2.536503  1615.570632  22.266912  10.304450  5.350487  1.0\n",
       "6   5.171688  3.692662  1500.644337  19.560275   6.533405  5.585594  1.0\n",
       "7   5.274171  3.884380  1713.973949  18.219146   4.113285  5.730975  1.0\n",
       "8   5.386611  3.890295  1793.920032  37.061753  21.421566  4.719471  1.0\n",
       "9   4.746015  3.890763  1796.553582  29.500502  19.236215  4.750329  1.0\n",
       "10  4.466423  3.810704  1466.874357  32.690355  20.672849  3.931757  2.0\n",
       "11  4.106173  3.867734  1575.923374  26.707565  19.672221  3.646650  2.0\n",
       "12  3.919363  3.877104  1790.886503  19.093613  13.706893  3.200008  2.0\n",
       "13  3.869127  3.883059  1798.879986  15.557579   9.145337  3.336426  2.0\n",
       "14  3.581195  3.886863  1798.610998  13.296674   5.122201  3.472623  2.0\n",
       "15  5.158961  3.571101  1684.204426  24.509088  14.637121  5.503001  3.0\n",
       "16  4.725500  3.879816  1718.812138  25.946059  17.410468  5.270173  3.0\n",
       "17  4.552518  3.887147  1733.439758  19.313818  13.009885  4.748834  3.0\n",
       "18  4.421236  3.888974  1799.752302  14.996562   7.869254  4.218462  3.0\n",
       "19  4.309075  3.889969  1789.924771  18.096969  12.767435  4.183788  3.0\n",
       "20  5.194257  3.678310  1647.538984  32.464651  20.549492  4.819023  4.0\n",
       "21  4.468170  3.854944  1672.027054  24.103537  18.933053  4.117983  4.0\n",
       "22  4.164059  3.881997  1791.354796  19.461623  14.317987  3.594925  4.0\n",
       "23  4.116176  3.885858  1799.326410  14.230771   7.399906  3.622363  4.0\n",
       "24  4.188899  3.887815  1798.977347  13.925725   6.970663  3.759375  4.0\n",
       "25  4.919832  3.766389  1676.027498  25.793602  14.995999  5.385042  5.0\n",
       "26  4.810830  3.875770  1472.906902  21.369425  14.858473  5.396352  5.0\n",
       "27  4.425331  3.887002  1758.440117  25.071947  17.726569  4.502183  5.0\n",
       "28  4.377883  3.888670  1798.792962  16.123828   9.661445  3.901230  5.0\n",
       "29  4.351032  3.889489  1800.632763  13.811703   6.990955  3.840350  5.0\n",
       "30  4.465528  3.804899  1321.393377  26.113540  15.097145  5.465224  6.0\n",
       "31  4.467101  3.875893  1498.503158  24.173282  16.311282  5.031290  6.0\n",
       "32  4.279537  3.885512  1760.515531  20.644871  14.062718  4.065789  6.0\n",
       "33  4.359911  3.887612  1797.532257  15.220594   7.887485  3.872348  6.0\n",
       "34  4.308794  3.888934  1798.677690  13.170182   5.395205  3.879461  6.0\n",
       "35  5.144594  3.679558  1478.121364  34.118477  20.720334  5.314880  7.0\n",
       "36  5.146727  2.342519  1687.876822  23.157004  19.084775  4.636912  7.0\n",
       "37  5.249733  3.241539  1268.168002  25.596265  19.470906  4.116395  7.0\n",
       "38  5.221779  3.850586  1612.037400  21.496117  17.036536  3.705089  7.0\n",
       "39  4.926059  3.880613  1782.266255  18.360120  12.254652  3.634124  7.0\n",
       "40  5.479139  2.356116  1605.777898  33.256154  20.417833  4.522511  8.0\n",
       "41  5.563971  2.716440  1449.737472  31.228890  19.960433  4.383212  8.0\n",
       "42  5.716483  3.882619  1598.946421  25.510939  16.877881  4.805733  8.0\n",
       "43  5.692486  3.890131  1784.283320  22.409438  13.657009  5.028234  8.0\n",
       "44  5.629410  3.890950  1800.954830  18.187672   9.569073  4.098025  8.0\n",
       "45  4.462962  3.799878  1369.781138  32.361093  20.411359  4.967579  9.0\n",
       "46  4.196876  3.868982  1730.332760  29.434293  20.171021  3.881857  9.0\n",
       "47  4.080861  3.880762  1789.269900  19.269377  14.249326  3.500343  9.0\n",
       "48  4.052663  3.884902  1798.900774  13.704530   6.167322  3.622772  9.0\n",
       "49  4.068037  3.887472  1798.061547  13.229344   5.492635  3.763575  9.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create results dataframe\n",
    "synthetic_trips_df = pd.DataFrame(synthetic_chains)\n",
    "synthetic_trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict fewer than 1,000 characters as output for a given seed.\n",
    "# Remove all punctuation from the source text, and therefore from the modelsâ€™ vocabulary.\n",
    "# Try a one hot encoded for the input sequences.\n",
    "# Train the model on padded sentences rather than random sequences of characters.\n",
    "# Increase the number of training epochs to 100 or many hundreds.\n",
    "# Add dropout to the visible input layer and consider tuning the dropout percentage.\n",
    "# Tune the batch size, try a batch size of 1 as a (very slow) baseline and larger sizes from there.\n",
    "# Add more memory units to the layers and/or more layers.\n",
    "# Experiment with scale factors (temperature) when interpreting the prediction probabilities.\n",
    "# Change the LSTM layers to be â€œstatefulâ€ to maintain state across batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
